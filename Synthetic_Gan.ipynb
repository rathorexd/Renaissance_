{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbwpHR4TX9d2",
        "outputId": "85a60e13-1b86-4762-d59f-bc39c77a843a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Hir1cZFiAak",
        "outputId": "c90c9457-88ba-44c9-f7da-10f081f0349d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tesseract-ocr-spa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0zwOqINc99O",
        "outputId": "c6fa4a09-b37d-4b04-854f-02c7da637c5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd tesseract-ocr-spa\n",
            "0 upgraded, 4 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 5,768 kB of archives.\n",
            "After this operation, 17.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-spa all 1:4.00~git30-7274cfa-1.1 [951 kB]\n",
            "Fetched 5,768 kB in 1s (4,915 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package tesseract-ocr-spa.\n",
            "Preparing to unpack .../tesseract-ocr-spa_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-spa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-spa (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur3G-hP8dBJu",
        "outputId": "2d94243b-5a4b-43db-e961-f169f002e684"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pytesseract\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "fXnsqTmAcaWj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def extract_text_with_tesseract(pdf_path):\n",
        "    import fitz  # PyMuPDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        pix = page.get_pixmap()  # Render page as an image\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "        # Use Tesseract to extract text from the image\n",
        "        text += pytesseract.image_to_string(img, lang='spa')  # 'spa' = Spanish\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "pdf_path = \"/content/Ezcaray - Vozes.pdf\"\n",
        "extracted_text = extract_text_with_tesseract(pdf_path)\n",
        "\n",
        "# Save the extracted text\n",
        "with open(\"extracted_text_tesseract.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(extracted_text)\n",
        "\n",
        "print(\"Text extracted using Tesseract and saved to 'extracted_text_tesseract.txt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyM03nIYZMXh",
        "outputId": "ec3a6cd3-16b5-4f75-955f-fafff26671db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extracted using Tesseract and saved to 'extracted_text_tesseract.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Normalize spaces and remove unwanted characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?¿¡()]', '', text)  # Remove special characters\n",
        "    return text.strip()\n",
        "\n",
        "# Load the extracted text\n",
        "with open(\"extracted_text_tesseract.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Clean the text\n",
        "cleaned_text = clean_text(text)\n",
        "print(cleaned_text[:500])  # Print the first 500 characters to verify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oimGd_RmdYvj",
        "outputId": "8b24d3a0-974e-4f21-a16c-695eda610ff7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEÑOR ILVSTRISSIMO. Y Cupado en el exércicio de las Mifliones en el Obifpado de Guadala xararecibiwna de V.S:L. 5. len que nté da noticia de como fu Mageltad (que Dios guarde) fe avia fervido de honrarme con la merced de fu Predicador; y como no e opone la predicación de fu Magefe tad ala Apoftolica,tuvé pordemiobli gaciónadmitir el fávor, rindiendo a V.S.Lel agradecimiento. El Rey mi feñor ( que Dios guarde) hizo la gracia ; mas aV.S.K. fele deber que por mas frutos; que didra la tierra! de Pro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def render_text_image(text, font_path, output_image_path, image_size=(512, 512)):\n",
        "    # Create a blank image\n",
        "    image = Image.new(\"L\", image_size, color=255)  # White background\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    #Renaissance-style font\n",
        "    font = ImageFont.truetype(font_path, size=40)\n",
        "\n",
        "    # Draw the text\n",
        "    draw.text((10, 10), text, font=font, fill=0)  # Black text\n",
        "    image.save(output_image_path)\n",
        "\n",
        "# Example usage\n",
        "font_path = \"/content/IMFellEnglish-Regular.ttf\"\n",
        "output_image_path = \"rendered_text.png\"\n",
        "render_text_image(cleaned_text, font_path, output_image_path)  # Render first 500 characters"
      ],
      "metadata": {
        "id": "6lk6nT8seLfz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def add_imperfections(image_path, output_image_path):\n",
        "\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Ink bleed\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    image = cv2.dilate(image, kernel, iterations=1)\n",
        "\n",
        "    # Smudging (Gaussian blur)\n",
        "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    # Faded text (adjust brightness and contrast)\n",
        "    alpha = 0.8  # Contrast control\n",
        "    beta = 50    # Brightness control\n",
        "    image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
        "\n",
        "    # Save the degraded image\n",
        "    cv2.imwrite(output_image_path, image)\n",
        "\n",
        "# Example usage\n",
        "degraded_image_path = \"degraded_text.png\"\n",
        "add_imperfections(output_image_path, degraded_image_path)"
      ],
      "metadata": {
        "id": "UA4RlBEL0Y8h"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(text, font_path, output_dir, num_samples=100):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    clean_dir = os.path.join(output_dir, \"clean\")\n",
        "    degraded_dir = os.path.join(output_dir, \"degraded\")\n",
        "    os.makedirs(clean_dir, exist_ok=True)\n",
        "    os.makedirs(degraded_dir, exist_ok=True)\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_chunks = [text[i:i+500] for i in range(0, len(text), 500)]  # 500 characters per chunk\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        chunk = text_chunks[i % len(text_chunks)]  # Cycle through the text chunks\n",
        "        clean_image_path = os.path.join(clean_dir, f\"{i}.png\")\n",
        "        degraded_image_path = os.path.join(degraded_dir, f\"{i}.png\")\n",
        "\n",
        "        # Render clean text image\n",
        "        render_text_image(chunk, font_path, clean_image_path)\n",
        "\n",
        "        # Add imperfections to create degraded image\n",
        "        add_imperfections(clean_image_path, degraded_image_path)\n",
        "\n",
        "    print(f\"Dataset created at {output_dir}\")\n",
        "\n",
        "create_dataset(text, font_path, \"dataset\", num_samples=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYImOjFO0l7-",
        "outputId": "d142d9b2-6b77-4e6e-9a9f-dd68afc0423c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created at dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(256, 256, 1))\n",
        "\n",
        "    # Encoder\n",
        "    e1 = layers.Conv2D(64, 4, strides=2, padding=\"same\")(inputs)\n",
        "    e1 = layers.LeakyReLU(alpha=0.2)(e1)\n",
        "\n",
        "    e2 = layers.Conv2D(128, 4, strides=2, padding=\"same\")(e1)\n",
        "    e2 = layers.BatchNormalization()(e2)\n",
        "    e2 = layers.LeakyReLU(alpha=0.2)(e2)\n",
        "\n",
        "    e3 = layers.Conv2D(256, 4, strides=2, padding=\"same\")(e2)\n",
        "    e3 = layers.BatchNormalization()(e3)\n",
        "    e3 = layers.LeakyReLU(alpha=0.2)(e3)\n",
        "\n",
        "    # Decoder\n",
        "    d1 = layers.Conv2DTranspose(128, 4, strides=2, padding=\"same\")(e3)\n",
        "    d1 = layers.BatchNormalization()(d1)\n",
        "    d1 = layers.ReLU()(d1)\n",
        "\n",
        "    d2 = layers.Conv2DTranspose(64, 4, strides=2, padding=\"same\")(d1)\n",
        "    d2 = layers.BatchNormalization()(d2)\n",
        "    d2 = layers.ReLU()(d2)\n",
        "\n",
        "    outputs = layers.Conv2DTranspose(1, 4, strides=2, padding=\"same\", activation=\"tanh\")(d2)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "generator = build_generator()\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "DDQIMg2b6pfG",
        "outputId": "47ca8d39-3487-4b9e-8f6c-4f7bf1522df8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m1,088\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m131,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m524,544\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_transpose (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m524,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_transpose_1 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m131,136\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_transpose_2 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │           \u001b[38;5;34m1,025\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_transpose (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_transpose_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,136</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_transpose_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,315,713\u001b[0m (5.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,713</span> (5.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,314,561\u001b[0m (5.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,314,561</span> (5.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,152\u001b[0m (4.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> (4.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(256, 256, 1))\n",
        "\n",
        "    x = layers.Conv2D(64, 4, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 4, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, 4, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    outputs = layers.Conv2D(1, 4, strides=1, padding=\"same\", activation=\"sigmoid\")(x)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "Az6tOV4VFa3e",
        "outputId": "aa3bec12-2df2-4a2c-ad39-2c1518fa8499"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m1,088\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m131,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m524,544\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │           \u001b[38;5;34m4,097\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,097</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m662,465\u001b[0m (2.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">662,465</span> (2.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m661,697\u001b[0m (2.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">661,697</span> (2.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss functions and optimizers\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "# Training loop\n",
        "def train_gan(generator, discriminator, dataset, epochs=100, batch_size=8):\n",
        "    for epoch in range(epochs):\n",
        "        for clean_batch, degraded_batch in dataset:\n",
        "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "                # Generate fake images\n",
        "                fake_images = generator(clean_batch, training=True)\n",
        "\n",
        "                # Discriminator output for real and fake images\n",
        "                real_output = discriminator(degraded_batch, training=True)\n",
        "                fake_output = discriminator(fake_images, training=True)\n",
        "\n",
        "                # Discriminator loss\n",
        "                real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "                fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "                disc_loss = real_loss + fake_loss\n",
        "\n",
        "                # Generator loss\n",
        "                gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "            # Apply gradients\n",
        "            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "\n",
        "            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "            generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Discriminator Loss: {disc_loss}, Generator Loss: {gen_loss}\")\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(clean_dir, degraded_dir, batch_size=8):\n",
        "    clean_images = [os.path.join(clean_dir, fname) for fname in os.listdir(clean_dir)]\n",
        "    degraded_images = [os.path.join(degraded_dir, fname) for fname in os.listdir(degraded_dir)]\n",
        "\n",
        "    def preprocess_image(image_path):\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_png(image, channels=1)\n",
        "        image = tf.image.resize(image, [256, 256])\n",
        "        image = tf.cast(image, tf.float32) / 127.5 - 1  # Normalize to [-1, 1]\n",
        "        return image\n",
        "\n",
        "    clean_dataset = tf.data.Dataset.from_tensor_slices(clean_images).map(preprocess_image).batch(batch_size)\n",
        "    degraded_dataset = tf.data.Dataset.from_tensor_slices(degraded_images).map(preprocess_image).batch(batch_size)\n",
        "\n",
        "    return tf.data.Dataset.zip((clean_dataset, degraded_dataset))\n",
        "\n",
        "# Train the GAN\n",
        "dataset = load_dataset(\"dataset/clean\", \"dataset/degraded\")\n",
        "train_gan(generator, discriminator, dataset, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9M5DSirFfRQ",
        "outputId": "588bc56d-ec1a-43db-90cb-175e0d1e6411"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Discriminator Loss: 0.7101912498474121, Generator Loss: 1.4519680738449097\n",
            "Epoch 2, Discriminator Loss: 0.5158618092536926, Generator Loss: 1.7217036485671997\n",
            "Epoch 3, Discriminator Loss: 1.328108787536621, Generator Loss: 0.8711020946502686\n",
            "Epoch 4, Discriminator Loss: 1.103243350982666, Generator Loss: 0.9728996753692627\n",
            "Epoch 5, Discriminator Loss: 0.993140459060669, Generator Loss: 1.0620049238204956\n",
            "Epoch 6, Discriminator Loss: 0.9718620777130127, Generator Loss: 1.0968456268310547\n",
            "Epoch 7, Discriminator Loss: 0.7525038719177246, Generator Loss: 1.3407453298568726\n",
            "Epoch 8, Discriminator Loss: 0.8089567422866821, Generator Loss: 1.3741068840026855\n",
            "Epoch 9, Discriminator Loss: 0.7476050853729248, Generator Loss: 1.3715591430664062\n",
            "Epoch 10, Discriminator Loss: 1.0373213291168213, Generator Loss: 1.3606370687484741\n",
            "Epoch 11, Discriminator Loss: 0.9997538328170776, Generator Loss: 1.1966084241867065\n",
            "Epoch 12, Discriminator Loss: 0.8503906726837158, Generator Loss: 1.3282899856567383\n",
            "Epoch 13, Discriminator Loss: 0.9630655646324158, Generator Loss: 1.2249672412872314\n",
            "Epoch 14, Discriminator Loss: 0.7995177507400513, Generator Loss: 1.2209575176239014\n",
            "Epoch 15, Discriminator Loss: 0.8414825797080994, Generator Loss: 1.2348400354385376\n",
            "Epoch 16, Discriminator Loss: 0.8365819454193115, Generator Loss: 1.3136987686157227\n",
            "Epoch 17, Discriminator Loss: 0.6833912134170532, Generator Loss: 1.3492006063461304\n",
            "Epoch 18, Discriminator Loss: 0.9154389500617981, Generator Loss: 0.9940718412399292\n",
            "Epoch 19, Discriminator Loss: 0.7004885673522949, Generator Loss: 1.6780412197113037\n",
            "Epoch 20, Discriminator Loss: 0.6506221294403076, Generator Loss: 1.543464183807373\n",
            "Epoch 21, Discriminator Loss: 0.8309253454208374, Generator Loss: 1.186331033706665\n",
            "Epoch 22, Discriminator Loss: 0.8095066547393799, Generator Loss: 1.285682201385498\n",
            "Epoch 23, Discriminator Loss: 0.808125376701355, Generator Loss: 1.257432222366333\n",
            "Epoch 24, Discriminator Loss: 0.8981574773788452, Generator Loss: 1.2945154905319214\n",
            "Epoch 25, Discriminator Loss: 0.8581055402755737, Generator Loss: 1.1148713827133179\n",
            "Epoch 26, Discriminator Loss: 0.7340024709701538, Generator Loss: 1.4494860172271729\n",
            "Epoch 27, Discriminator Loss: 0.9866803288459778, Generator Loss: 1.146755337715149\n",
            "Epoch 28, Discriminator Loss: 0.9751801490783691, Generator Loss: 1.529880404472351\n",
            "Epoch 29, Discriminator Loss: 0.8255540132522583, Generator Loss: 1.3284351825714111\n",
            "Epoch 30, Discriminator Loss: 1.1795451641082764, Generator Loss: 0.8878142833709717\n",
            "Epoch 31, Discriminator Loss: 1.0109037160873413, Generator Loss: 1.1268398761749268\n",
            "Epoch 32, Discriminator Loss: 1.0781522989273071, Generator Loss: 1.2996776103973389\n",
            "Epoch 33, Discriminator Loss: 0.9049053192138672, Generator Loss: 1.267855167388916\n",
            "Epoch 34, Discriminator Loss: 1.0167357921600342, Generator Loss: 1.111581802368164\n",
            "Epoch 35, Discriminator Loss: 1.044998049736023, Generator Loss: 1.0621157884597778\n",
            "Epoch 36, Discriminator Loss: 0.9608379006385803, Generator Loss: 1.071040391921997\n",
            "Epoch 37, Discriminator Loss: 1.0247111320495605, Generator Loss: 1.3184330463409424\n",
            "Epoch 38, Discriminator Loss: 0.9428263306617737, Generator Loss: 1.037736415863037\n",
            "Epoch 39, Discriminator Loss: 1.1554265022277832, Generator Loss: 0.7968298196792603\n",
            "Epoch 40, Discriminator Loss: 1.16618013381958, Generator Loss: 0.9988227486610413\n",
            "Epoch 41, Discriminator Loss: 0.9599491953849792, Generator Loss: 0.8784278035163879\n",
            "Epoch 42, Discriminator Loss: 1.0383491516113281, Generator Loss: 0.8505882024765015\n",
            "Epoch 43, Discriminator Loss: 1.0330379009246826, Generator Loss: 0.7510419487953186\n",
            "Epoch 44, Discriminator Loss: 1.0786304473876953, Generator Loss: 0.8787522912025452\n",
            "Epoch 45, Discriminator Loss: 0.9833195209503174, Generator Loss: 0.8851740956306458\n",
            "Epoch 46, Discriminator Loss: 1.32891845703125, Generator Loss: 0.6713924407958984\n",
            "Epoch 47, Discriminator Loss: 0.8623562455177307, Generator Loss: 1.1380560398101807\n",
            "Epoch 48, Discriminator Loss: 1.2575340270996094, Generator Loss: 0.7608307600021362\n",
            "Epoch 49, Discriminator Loss: 0.9768755435943604, Generator Loss: 1.19581937789917\n",
            "Epoch 50, Discriminator Loss: 1.2171928882598877, Generator Loss: 0.9075369834899902\n",
            "Epoch 51, Discriminator Loss: 1.1485440731048584, Generator Loss: 1.0440102815628052\n",
            "Epoch 52, Discriminator Loss: 1.1025710105895996, Generator Loss: 1.3717608451843262\n",
            "Epoch 53, Discriminator Loss: 1.299782156944275, Generator Loss: 0.84688401222229\n",
            "Epoch 54, Discriminator Loss: 1.210497498512268, Generator Loss: 0.6852362751960754\n",
            "Epoch 55, Discriminator Loss: 1.098804235458374, Generator Loss: 0.8697397112846375\n",
            "Epoch 56, Discriminator Loss: 1.3471978902816772, Generator Loss: 0.7451528906822205\n",
            "Epoch 57, Discriminator Loss: 1.1860108375549316, Generator Loss: 0.7624824643135071\n",
            "Epoch 58, Discriminator Loss: 1.1987404823303223, Generator Loss: 0.7909111976623535\n",
            "Epoch 59, Discriminator Loss: 1.5395760536193848, Generator Loss: 0.3942340314388275\n",
            "Epoch 60, Discriminator Loss: 1.1657036542892456, Generator Loss: 0.8806350231170654\n",
            "Epoch 61, Discriminator Loss: 1.1530232429504395, Generator Loss: 0.9076238870620728\n",
            "Epoch 62, Discriminator Loss: 1.1059083938598633, Generator Loss: 1.2679781913757324\n",
            "Epoch 63, Discriminator Loss: 1.4393668174743652, Generator Loss: 0.43193358182907104\n",
            "Epoch 64, Discriminator Loss: 1.1961408853530884, Generator Loss: 0.9263169169425964\n",
            "Epoch 65, Discriminator Loss: 1.3108261823654175, Generator Loss: 0.8164939880371094\n",
            "Epoch 66, Discriminator Loss: 1.0941519737243652, Generator Loss: 1.0954606533050537\n",
            "Epoch 67, Discriminator Loss: 1.3566704988479614, Generator Loss: 0.5606181025505066\n",
            "Epoch 68, Discriminator Loss: 1.2610951662063599, Generator Loss: 0.7635998725891113\n",
            "Epoch 69, Discriminator Loss: 1.2158862352371216, Generator Loss: 0.8416657447814941\n",
            "Epoch 70, Discriminator Loss: 1.2508554458618164, Generator Loss: 0.9913531541824341\n",
            "Epoch 71, Discriminator Loss: 1.1076979637145996, Generator Loss: 0.8425479531288147\n",
            "Epoch 72, Discriminator Loss: 1.3171344995498657, Generator Loss: 1.069128394126892\n",
            "Epoch 73, Discriminator Loss: 1.3442877531051636, Generator Loss: 0.5288286805152893\n",
            "Epoch 74, Discriminator Loss: 1.185655117034912, Generator Loss: 0.7976327538490295\n",
            "Epoch 75, Discriminator Loss: 1.246518611907959, Generator Loss: 0.7638556361198425\n",
            "Epoch 76, Discriminator Loss: 1.3351528644561768, Generator Loss: 0.768338143825531\n",
            "Epoch 77, Discriminator Loss: 1.224636435508728, Generator Loss: 0.8207213878631592\n",
            "Epoch 78, Discriminator Loss: 1.2673691511154175, Generator Loss: 1.0657585859298706\n",
            "Epoch 79, Discriminator Loss: 1.204694151878357, Generator Loss: 0.7551179528236389\n",
            "Epoch 80, Discriminator Loss: 0.6558144688606262, Generator Loss: 1.9929542541503906\n",
            "Epoch 81, Discriminator Loss: 0.9982567429542542, Generator Loss: 1.42129385471344\n",
            "Epoch 82, Discriminator Loss: 1.1217176914215088, Generator Loss: 0.9339133501052856\n",
            "Epoch 83, Discriminator Loss: 1.0305943489074707, Generator Loss: 1.530854344367981\n",
            "Epoch 84, Discriminator Loss: 1.2159321308135986, Generator Loss: 0.5704970955848694\n",
            "Epoch 85, Discriminator Loss: 1.2287951707839966, Generator Loss: 1.1876829862594604\n",
            "Epoch 86, Discriminator Loss: 1.1375786066055298, Generator Loss: 1.67143976688385\n",
            "Epoch 87, Discriminator Loss: 1.0540986061096191, Generator Loss: 0.9007654190063477\n",
            "Epoch 88, Discriminator Loss: 1.7117595672607422, Generator Loss: 0.823589563369751\n",
            "Epoch 89, Discriminator Loss: 1.236212968826294, Generator Loss: 0.7961207628250122\n",
            "Epoch 90, Discriminator Loss: 1.1332063674926758, Generator Loss: 0.8726668953895569\n",
            "Epoch 91, Discriminator Loss: 1.2231080532073975, Generator Loss: 0.8490505218505859\n",
            "Epoch 92, Discriminator Loss: 1.2573351860046387, Generator Loss: 0.8448639512062073\n",
            "Epoch 93, Discriminator Loss: 1.336216926574707, Generator Loss: 0.7893751263618469\n",
            "Epoch 94, Discriminator Loss: 1.241304636001587, Generator Loss: 0.7144501209259033\n",
            "Epoch 95, Discriminator Loss: 1.2946453094482422, Generator Loss: 0.603016197681427\n",
            "Epoch 96, Discriminator Loss: 1.2391180992126465, Generator Loss: 0.7233758568763733\n",
            "Epoch 97, Discriminator Loss: 1.2347427606582642, Generator Loss: 0.9813769459724426\n",
            "Epoch 98, Discriminator Loss: 1.1570665836334229, Generator Loss: 0.8896229863166809\n",
            "Epoch 99, Discriminator Loss: 1.382059097290039, Generator Loss: 0.7440237998962402\n",
            "Epoch 100, Discriminator Loss: 1.2354958057403564, Generator Loss: 0.8362541198730469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_text(generator, clean_image_path):\n",
        "    clean_image = tf.io.read_file(clean_image_path)\n",
        "    clean_image = tf.image.decode_png(clean_image, channels=1)\n",
        "    clean_image = tf.image.resize(clean_image, [256, 256])\n",
        "    clean_image = tf.cast(clean_image, tf.float32) / 127.5 - 1  # Normalize to [-1, 1]\n",
        "    clean_image = tf.expand_dims(clean_image, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Generate degraded image\n",
        "    degraded_image = generator(clean_image, training=False)\n",
        "    degraded_image = (degraded_image[0].numpy() + 1) * 127.5  # Denormalize\n",
        "    degraded_image = np.clip(degraded_image, 0, 255).astype(np.uint8)\n",
        "    degraded_image = np.squeeze(degraded_image)  # Remove extra dimension\n",
        "\n",
        "\n",
        "    return degraded_image\n",
        "\n",
        "\n",
        "synthetic_image = generate_synthetic_text(generator, \"/content/rendered_text.png\")\n",
        "Image.fromarray(synthetic_image).save(\"synthetic_text.png\")"
      ],
      "metadata": {
        "id": "dizGd5luFrNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving models\n",
        "generator.save(\"generator_model.h5\")\n",
        "discriminator.save(\"discriminator_model.h5\")\n",
        "print(\"Generator and Discriminator models saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgiiSkAo297Y",
        "outputId": "5d0cdf95-1569-4ea4-aa1f-49fddd1d9b77"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator models saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#genrating synthetic texts from genrator\n"
      ],
      "metadata": {
        "id": "7g154UTIHDx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def render_text_image(text, font_path, output_image_path, image_size=(512, 512)):\n",
        "    # Create a blank image\n",
        "    image = Image.new(\"L\", image_size, color=255)  # White background\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Load a Renaissance-style font\n",
        "    font = ImageFont.truetype(font_path, size=40)\n",
        "\n",
        "    # Draw the text\n",
        "    draw.text((10, 10), text, font=font, fill=0)  # Black text\n",
        "    image.save(output_image_path)\n",
        "\n",
        "# Example usage\n",
        "font_path = \"/content/IMFellEnglish-Regular.ttf\"\n",
        "\n",
        "# Split the text into 5 chunks (pages)\n",
        "text_chunks = [cleaned_text[i:i+2000] for i in range(0, len(cleaned_text), 2000)]  # 2000 characters per page\n",
        "text_chunks = text_chunks[:5]  # Ensure only 5 pages\n",
        "\n",
        "# Render each chunk as an image\n",
        "for i, chunk in enumerate(text_chunks):\n",
        "    output_image_path = f\"clean_page_{i+1}.png\"\n",
        "    render_text_image(chunk, font_path, output_image_path)\n",
        "    print(f\"Rendered page {i+1} to {output_image_path}\")"
      ],
      "metadata": {
        "id": "tGgsKNCTXMa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacb4958-ab92-42f9-e659-5d8bcecb405b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendered page 1 to clean_page_1.png\n",
            "Rendered page 2 to clean_page_2.png\n",
            "Rendered page 3 to clean_page_3.png\n",
            "Rendered page 4 to clean_page_4.png\n",
            "Rendered page 5 to clean_page_5.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_synthetic_text(generator, clean_image_path):\n",
        "    # Load and preprocess the clean image\n",
        "    clean_image = tf.io.read_file(clean_image_path)\n",
        "    clean_image = tf.image.decode_png(clean_image, channels=1)\n",
        "    clean_image = tf.image.resize(clean_image, [256, 256])\n",
        "    clean_image = tf.cast(clean_image, tf.float32) / 127.5 - 1  # Normalize to [-1, 1]\n",
        "    clean_image = tf.expand_dims(clean_image, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Generate degraded image\n",
        "    degraded_image = generator(clean_image, training=False)\n",
        "    degraded_image = (degraded_image[0].numpy() + 1) * 127.5  # Denormalize\n",
        "    degraded_image = np.clip(degraded_image, 0, 255).astype(np.uint8)\n",
        "    degraded_image = np.squeeze(degraded_image)  # Remove extra dimension\n",
        "\n",
        "\n",
        "    return degraded_image\n",
        "\n",
        "# Generate synthetic data for 5 pages\n",
        "for i in range(5):\n",
        "    clean_image_path = f\"clean_page_{i+1}.png\"\n",
        "    synthetic_image = generate_synthetic_text(generator, clean_image_path)\n",
        "\n",
        "    synthetic_image_path = f\"synthetic_page_{i+1}.png\"\n",
        "    Image.fromarray(synthetic_image).save(synthetic_image_path)\n",
        "    print(f\"Generated synthetic page {i+1} and saved to {synthetic_image_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMJFw6yz1Prx",
        "outputId": "6384151a-9ef3-4c17-c508-fb0c168c2026"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated synthetic page 1 and saved to synthetic_page_1.png\n",
            "Generated synthetic page 2 and saved to synthetic_page_2.png\n",
            "Generated synthetic page 3 and saved to synthetic_page_3.png\n",
            "Generated synthetic page 4 and saved to synthetic_page_4.png\n",
            "Generated synthetic page 5 and saved to synthetic_page_5.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXMVwYoh6YRy",
        "outputId": "bac365b1-32b6-4452-9aaf-e71e97294e93"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=4580514a7fbd8f37d3a7d2641cc1177e4313f5c177fc4153ec062b61ad346c00\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "# Create a PDF\n",
        "pdf = FPDF()\n",
        "\n",
        "# Add each synthetic page to the PDF\n",
        "for i in range(5):\n",
        "    synthetic_image_path = f\"synthetic_page_{i+1}.png\"\n",
        "    pdf.add_page()\n",
        "    pdf.image(synthetic_image_path, x=10, y=10, w=180)\n",
        "\n",
        "# Save the PDF\n",
        "pdf.output(\"synthetic_renaissance_text.pdf\")\n",
        "print(\"Saved 5 pages of synthetic data to 'synthetic_renaissance_text.pdf'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXvuRxDG5jLX",
        "outputId": "5c3a7ba0-8067-49bf-8fdb-17ccc5677089"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 pages of synthetic data to 'synthetic_renaissance_text.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# genrating synthetic renassiance paragraphs\n"
      ],
      "metadata": {
        "id": "c5hvPt3UHP7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\"“SEÑOR ILVSTRISSIMO.\n",
        "\n",
        "Y Cupado en el exércicio\n",
        "de las Mifliones en el\n",
        "Obifpado de Guadala-\n",
        "xararecibiwna de V.S:L.\n",
        "\n",
        "5. len que nté da noticia de\n",
        "como fu Mageltad (que Dios guarde)\n",
        "fe avia fervido de honrarme con la\n",
        "merced de fu Predicador; y como no\n",
        "e opone la predicación de fu Magefe\n",
        "tad ala Apoftolica,tuvé pordemiobli\n",
        "gación'admitir el fávor, rindiendo a\n",
        "V.S.L'el agradecimiento.\n",
        "\n",
        "El Rey mi feñor ( que Dios guarde)\n",
        "hizo la gracia ; mas aV.S.K. fele deber\n",
        "que por mas frutos; que didra la tierra!\n",
        "de Promiffion,no los lograra Moyfes,\n",
        "fi Jofue,y Caleb no los fácaflen: Dos\n",
        "fácaron el fruto, y de ambos neceffito,\n",
        "para hallar.vn fimil propotcionado a la\n",
        "grandeza de V.S.I,\"\"\"\n",
        "\n",
        "# Split text into paragraphs\n",
        "paragraphs = text.strip().split(\"\\n\\n\")  # Split by double newlines\n",
        "print(f\"Number of paragraphs: {len(paragraphs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-USJSE98Br-",
        "outputId": "60f57fb6-eac2-4bf0-8171-b951a12f7d6b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of paragraphs: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def render_text_image(text, font_path, output_image_path, image_size=(512, 512)):\n",
        "    # Create a blank image\n",
        "    image = Image.new(\"L\", image_size, color=255)  # White background\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Load a Renaissance-style font\n",
        "    font = ImageFont.truetype(font_path, size=40)\n",
        "\n",
        "    # Draw the text\n",
        "    draw.text((10, 10), text, font=font, fill=0)  # Black text\n",
        "    image.save(output_image_path)\n",
        "\n",
        "font_path = \"/content/IMFellEnglish-Regular.ttf\"\n",
        "# Render each paragraph as an image\n",
        "for i, paragraph in enumerate(paragraphs):\n",
        "    output_image_path = f\"clean_paragraph_{i+1}.png\"\n",
        "    render_text_image(paragraph, font_path, output_image_path)\n",
        "    print(f\"Rendered paragraph {i+1} to {output_image_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtW9IUAa8L48",
        "outputId": "96d7c883-ac11-4cc1-84b2-747e7892020d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rendered paragraph 1 to clean_paragraph_1.png\n",
            "Rendered paragraph 2 to clean_paragraph_2.png\n",
            "Rendered paragraph 3 to clean_paragraph_3.png\n",
            "Rendered paragraph 4 to clean_paragraph_4.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def add_imperfections(image_path, output_image_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Ink bleed (dilation)\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    image = cv2.dilate(image, kernel, iterations=1)\n",
        "\n",
        "    # Smudging (Gaussian blur)\n",
        "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    # Faded text (adjust brightness and contrast)\n",
        "    alpha = 0.8  # Contrast control\n",
        "    beta = 50    # Brightness control\n",
        "    image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
        "\n",
        "    # Save the degraded image\n",
        "    cv2.imwrite(output_image_path, image)\n",
        "\n",
        "# Add imperfections to each paragraph image\n",
        "for i in range(len(paragraphs)):\n",
        "    clean_image_path = f\"clean_paragraph_{i+1}.png\"\n",
        "    degraded_image_path = f\"degraded_paragraph_{i+1}.png\"\n",
        "    add_imperfections(clean_image_path, degraded_image_path)\n",
        "    print(f\"Added imperfections to paragraph {i+1} and saved to {degraded_image_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO7gKU048Yta",
        "outputId": "1e0e614b-083e-4b92-8ab7-e5b772729e5e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added imperfections to paragraph 1 and saved to degraded_paragraph_1.png\n",
            "Added imperfections to paragraph 2 and saved to degraded_paragraph_2.png\n",
            "Added imperfections to paragraph 3 and saved to degraded_paragraph_3.png\n",
            "Added imperfections to paragraph 4 and saved to degraded_paragraph_4.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def generate_synthetic_text(generator, clean_image_path):\n",
        "    # Load and preprocess the clean image\n",
        "    clean_image = tf.io.read_file(clean_image_path)\n",
        "    clean_image = tf.image.decode_png(clean_image, channels=1)\n",
        "    clean_image = tf.image.resize(clean_image, [256, 256])\n",
        "    clean_image = tf.cast(clean_image, tf.float32) / 127.5 - 1  # Normalize to [-1, 1]\n",
        "    clean_image = tf.expand_dims(clean_image, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Generate degraded image\n",
        "    degraded_image = generator(clean_image, training=False)\n",
        "    degraded_image = (degraded_image[0].numpy() + 1) * 127.5  # Denormalize\n",
        "    degraded_image = np.clip(degraded_image, 0, 255).astype(np.uint8)\n",
        "    degraded_image = np.squeeze(degraded_image)  # Remove extra dimension\n",
        "\n",
        "    return degraded_image\n",
        "\n",
        "# Generate synthetic data for each paragraph\n",
        "for i in range(len(paragraphs)):\n",
        "    clean_image_path = f\"clean_paragraph_{i+1}.png\"\n",
        "    synthetic_image = generate_synthetic_text(generator, clean_image_path)\n",
        "\n",
        "    # Save the synthetic image\n",
        "    synthetic_image_path = f\"synthetic_paragraph_{i+1}.png\"\n",
        "    Image.fromarray(synthetic_image).save(synthetic_image_path)\n",
        "    print(f\"Generated synthetic paragraph {i+1} and saved to {synthetic_image_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1t_9jXC8uJF",
        "outputId": "7062a678-12c5-4ebf-a17d-cdedaa1c67d8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated synthetic paragraph 1 and saved to synthetic_paragraph_1.png\n",
            "Generated synthetic paragraph 2 and saved to synthetic_paragraph_2.png\n",
            "Generated synthetic paragraph 3 and saved to synthetic_paragraph_3.png\n",
            "Generated synthetic paragraph 4 and saved to synthetic_paragraph_4.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "\n",
        "pdf = FPDF()\n",
        "\n",
        "# Add each synthetic paragraph to the PDF\n",
        "for i in range(len(paragraphs)):\n",
        "    synthetic_image_path = f\"synthetic_paragraph_{i+1}.png\"\n",
        "    pdf.add_page()\n",
        "    pdf.image(synthetic_image_path, x=10, y=10, w=180)  # Adjust dimensions as needed\n",
        "\n",
        "# Save the PDF\n",
        "pdf.output(\"synthetic_renaissance_paragraphs.pdf\")\n",
        "print(\"Saved synthetic paragraphs to 'synthetic_renaissance_paragraphs.pdf'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpaZ8x5h804u",
        "outputId": "e0991d73-1411-4169-b534-81fca6e9ac9a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved synthetic paragraphs to 'synthetic_renaissance_paragraphs.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import cv2\n",
        "\n",
        "def calculate_ssim_psnr(real_image_path, generated_image_path):\n",
        "    # Load images\n",
        "    real_image = cv2.imread(real_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    generated_image = cv2.imread(generated_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    #  both images should have the same dimensions\n",
        "    if real_image.shape != generated_image.shape:\n",
        "        generated_image = cv2.resize(generated_image, (real_image.shape[1], real_image.shape[0]))\n",
        "\n",
        "    # Calculate SSIM\n",
        "    ssim_value = ssim(real_image, generated_image, data_range=255)\n",
        "\n",
        "    # Calculate PSNR\n",
        "    psnr_value = psnr(real_image, generated_image, data_range=255)\n",
        "\n",
        "    return ssim_value, psnr_value\n",
        "\n",
        "# Example usage\n",
        "real_image_path = \"/content/degraded_paragraph_1.png\"\n",
        "generated_image_path = \"/content/synthetic_paragraph_1.png\"\n",
        "ssim_value, psnr_value = calculate_ssim_psnr(real_image_path, generated_image_path)\n",
        "print(f\"SSIM: {ssim_value}, PSNR: {psnr_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnIlP4oKHs_5",
        "outputId": "11ebf3b8-b10f-49c0-e719-db51cedf7c24"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSIM: 0.9922099064594826, PSNR: 38.7378163889172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEBeWm84I4nL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}